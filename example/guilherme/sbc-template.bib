
@article{sinaci_privacy-preserving_2024,
	title = {Privacy-preserving federated machine learning on {FAIR} health data: {A} real-world application},
	volume = {24},
	issn = {20010370},
	shorttitle = {Privacy-preserving federated machine learning on {FAIR} health data},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2001037024000382},
	doi = {10.1016/j.csbj.2024.02.014},
	language = {en},
	urldate = {2025-10-20},
	journal = {Computational and Structural Biotechnology Journal},
	author = {Sinaci, A. Anil and Gencturk, Mert and Alvarez-Romero, Celia and Laleci Erturkmen, Gokce Banu and Martinez-Garcia, Alicia and Escalona-Cuaresma, María José and Parra-Calderon, Carlos Luis},
	month = dec,
	year = {2024},
	pages = {136--145},
	file = {Texto completo:C\:\\Users\\guiba\\Zotero\\storage\\8X25Z74Y\\Sinaci et al. - 2024 - Privacy-preserving federated machine learning on FAIR health data A real-world application.pdf:application/pdf},
}

@article{yurdem_federated_2024,
	title = {Federated learning: {Overview}, strategies, applications, tools and future directions},
	volume = {10},
	issn = {24058440},
	shorttitle = {Federated learning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405844024141680},
	doi = {10.1016/j.heliyon.2024.e38137},
	language = {en},
	number = {19},
	urldate = {2025-10-20},
	journal = {Heliyon},
	author = {Yurdem, Betul and Kuzlu, Murat and Gullu, Mehmet Kemal and Catak, Ferhat Ozgur and Tabassum, Maliha},
	month = oct,
	year = {2024},
	keywords = {Lido},
	pages = {e38137},
	annote = {O paper dá um overview, básico, sem muito detalhe.
},
	file = {PDF:C\:\\Users\\guiba\\Zotero\\storage\\IIF7AEBU\\Yurdem et al. - 2024 - Federated learning Overview, strategies, applications, tools and future directions.pdf:application/pdf},
}

@article{wu_personalized_2020,
	title = {Personalized {Federated} {Learning} for {Intelligent} {IoT} {Applications}: {A} {Cloud}-{Edge} {Based} {Framework}},
	volume = {1},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2644-1268},
	shorttitle = {Personalized {Federated} {Learning} for {Intelligent} {IoT} {Applications}},
	url = {https://ieeexplore.ieee.org/document/9090366/},
	doi = {10.1109/OJCS.2020.2993259},
	urldate = {2025-10-20},
	journal = {IEEE Open Journal of the Computer Society},
	author = {Wu, Qiong and He, Kaiwen and Chen, Xu},
	year = {2020},
	pages = {35--44},
	file = {Texto completo:C\:\\Users\\guiba\\Zotero\\storage\\AA4XRJ78\\Wu et al. - 2020 - Personalized Federated Learning for Intelligent IoT Applications A Cloud-Edge Based Framework.pdf:application/pdf},
}

@article{pal_practical_2023,
	title = {Practical {Evaluation} of {Federated} {Learning} in {Edge} {AI} for {IoT}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by-sa/4.0},
	issn = {2549-9904, 2549-9610},
	url = {http://joiv.org/index.php/joiv/article/view/2329},
	doi = {10.30630/joiv.7.3-2.2329},
	abstract = {AI running locally on IoT Edge devices is called Edge AI. Federated Learning (FL) is a Machine Learning (ML) technique that builds upon the concept of distributed computing and preserves data privacy while still supporting trainable AI models. This paper evaluates the FL regarding practical CPU usage and training time. Additionally, the paper presents how biased IoT Edge clients affect the performance of an AI model. Existing literature on the performance of FL indicates that it is sensitive to imbalanced data distributions and does not easily converge in the presence of heterogeneous data. Furthermore, model training uses significant on-device resources, and low-power IoT devices cannot train complex ML models. This paper investigates optimal training parameters to make FL more performant and researches the use of model compression to make FL more accessible to IoT Edge devices. First, a flexible test environment is created that can emulate clients with biased data samples. Each compressed version of the ML model is used for FL. Evaluation is done regarding resources used and the overall ML model performance. Our current study shows an accuracy improvement of 1.16\% from modifying training parameters, but a balance is needed to prevent overfitting. Model compression can reduce resource usage by 5.42\% but tends to accelerate overfitting and increase model loss by 9.35\%.},
	number = {3-2},
	urldate = {2025-10-20},
	journal = {JOIV : International Journal on Informatics Visualization},
	author = {Pal, Sauryadeep and Umair, Muhammad and Tan, Wooi-Haw and Foo, Yee-Loo},
	month = nov,
	year = {2023},
	pages = {2115},
	file = {Full Text PDF:C\:\\Users\\guiba\\Zotero\\storage\\N546CM96\\Pal et al. - 2023 - Practical Evaluation of Federated Learning in Edge AI for IoT.pdf:application/pdf},
}

@misc{li_federated_2018,
	title = {Federated {Optimization} in {Heterogeneous} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1812.06127},
	doi = {10.48550/ARXIV.1812.06127},
	abstract = {Federated Learning is a distributed learning paradigm with two key challenges that differentiate it from traditional distributed optimization: (1) significant variability in terms of the systems characteristics on each device in the network (systems heterogeneity), and (2) non-identically distributed data across the network (statistical heterogeneity). In this work, we introduce a framework, FedProx, to tackle heterogeneity in federated networks. FedProx can be viewed as a generalization and re-parametrization of FedAvg, the current state-of-the-art method for federated learning. While this re-parameterization makes only minor modifications to the method itself, these modifications have important ramifications both in theory and in practice. Theoretically, we provide convergence guarantees for our framework when learning over data from non-identical distributions (statistical heterogeneity), and while adhering to device-level systems constraints by allowing each participating device to perform a variable amount of work (systems heterogeneity). Practically, we demonstrate that FedProx allows for more robust convergence than FedAvg across a suite of realistic federated datasets. In particular, in highly heterogeneous settings, FedProx demonstrates significantly more stable and accurate convergence behavior relative to FedAvg---improving absolute test accuracy by 22\% on average.},
	urldate = {2025-10-20},
	publisher = {arXiv},
	author = {Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
	year = {2018},
	note = {Version Number: 5},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
	annote = {Other
MLSys 2020},
	file = {PDF:C\:\\Users\\guiba\\Zotero\\storage\\DWDL82NW\\Li et al. - 2018 - Federated Optimization in Heterogeneous Networks.pdf:application/pdf},
}

@misc{noauthor_flower_nodate,
	title = {Flower},
	url = {https://flower.ai/docs/framework/main/en/index.html},
}

@misc{beutel_flower_2020,
	title = {Flower: {A} {Friendly} {Federated} {Learning} {Research} {Framework}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Flower},
	url = {https://arxiv.org/abs/2007.14390},
	doi = {10.48550/ARXIV.2007.14390},
	abstract = {Federated Learning (FL) has emerged as a promising technique for edge devices to collaboratively learn a shared prediction model, while keeping their training data on the device, thereby decoupling the ability to do machine learning from the need to store the data in the cloud. However, FL is difficult to implement realistically, both in terms of scale and systems heterogeneity. Although there are a number of research frameworks available to simulate FL algorithms, they do not support the study of scalable FL workloads on heterogeneous edge devices. In this paper, we present Flower -- a comprehensive FL framework that distinguishes itself from existing platforms by offering new facilities to execute large-scale FL experiments and consider richly heterogeneous FL device scenarios. Our experiments show Flower can perform FL experiments up to 15M in client size using only a pair of high-end GPUs. Researchers can then seamlessly migrate experiments to real devices to examine other parts of the design space. We believe Flower provides the community with a critical new tool for FL study and development.},
	urldate = {2025-10-20},
	publisher = {arXiv},
	author = {Beutel, Daniel J. and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Fernandez-Marques, Javier and Gao, Yan and Sani, Lorenzo and Li, Kwing Hei and Parcollet, Titouan and de Gusmão, Pedro Porto Buarque and Lane, Nicholas D.},
	year = {2020},
	note = {Version Number: 5},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Computer Vision and Pattern Recognition (cs.CV)},
	annote = {Other
Open-Source, mobile-friendly Federated Learning framework},
	file = {PDF:C\:\\Users\\guiba\\Zotero\\storage\\R44M6CKF\\Beutel et al. - 2020 - Flower A Friendly Federated Learning Research Framework.pdf:application/pdf},
}

@article{hatfaludi_foundational_2025,
	title = {Foundational {Models} and {Federated} {Learning}: {Survey}, {Taxonomy}, {Challenges} and {Practical} {Insights}},
	volume = {11},
	issn = {2376-5992},
	shorttitle = {Foundational {Models} and {Federated} {Learning}},
	url = {http://arxiv.org/abs/2509.05142},
	doi = {10.7717/peerj-cs.2993},
	abstract = {Federated learning has the potential to unlock siloed data and distributed resources by enabling collaborative model training without sharing private data. As more complex foundational models gain widespread use, the need to expand training resources and integrate privately owned data grows as well. In this article, we explore the intersection of federated learning and foundational models, aiming to identify, categorize, and characterize technical methods that integrate the two paradigms. As a unified survey is currently unavailable, we present a literature survey structured around a novel taxonomy that follows the development life-cycle stages, along with a technical comparison of available methods. Additionally, we provide practical insights and guidelines for implementing and evolving these methods, with a specific focus on the healthcare domain as a case study, where the potential impact of federated learning and foundational models is considered significant. Our survey covers multiple intersecting topics, including but not limited to federated learning, self-supervised learning, fine-tuning, distillation, and transfer learning. Initially, we retrieved and reviewed a set of over 4,200 articles. This collection was narrowed to more than 250 thoroughly reviewed articles through inclusion criteria, featuring 42 unique methods. The methods were used to construct the taxonomy and enabled their comparison based on complexity, efficiency, and scalability. We present these results as a self-contained overview that not only summarizes the state of the field but also provides insights into the practical aspects of adopting, evolving, and integrating foundational models with federated learning.},
	urldate = {2025-10-22},
	journal = {PeerJ Computer Science},
	author = {Hatfaludi, Cosmin-Andrei and Serban, Alex},
	month = jul,
	year = {2025},
	note = {arXiv:2509.05142 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {e2993},
	file = {Preprint PDF:C\:\\Users\\guiba\\Zotero\\storage\\DSJYD5UH\\Hatfaludi e Serban - 2025 - Foundational Models and Federated Learning Survey, Taxonomy, Challenges and Practical Insights.pdf:application/pdf;Snapshot:C\:\\Users\\guiba\\Zotero\\storage\\V6ZWYZY8\\2509.html:text/html},
}

@misc{fajardo_fedrag_2025,
	title = {{FedRAG}: {A} {Framework} for {Fine}-{Tuning} {Retrieval}-{Augmented} {Generation} {Systems}},
	shorttitle = {{FedRAG}},
	url = {http://arxiv.org/abs/2506.09200},
	doi = {10.48550/arXiv.2506.09200},
	abstract = {Retrieval-augmented generation (RAG) systems have been shown to be effective in addressing many of the drawbacks of relying solely on the parametric memory of large language models. Recent work has demonstrated that RAG systems can be improved via fine-tuning of their retriever and generator models. In this work, we introduce FedRAG, a framework for fine-tuning RAG systems across centralized and federated architectures. FedRAG supports state-of-the-art fine-tuning methods, offering a simple and intuitive interface and a seamless conversion from centralized to federated training tasks. FedRAG is also deeply integrated with the modern RAG ecosystem, filling a critical gap in available tools.},
	urldate = {2025-10-22},
	publisher = {arXiv},
	author = {Fajardo, Val Andrei and Emerson, David B. and Singh, Amandeep and Chatrath, Veronica and Lotif, Marcelo and Theja, Ravi and Cheung, Alex and Matsuba, Izuki},
	month = jun,
	year = {2025},
	note = {arXiv:2506.09200 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 9 pages, 4 figures, 2 tables. Accepted for the CODEML Workshop at ICML 2025. Framework code available at https://github.com/VectorInstitute/fed-rag},
	file = {Preprint PDF:C\:\\Users\\guiba\\Zotero\\storage\\Q4T2TH2Z\\Fajardo et al. - 2025 - FedRAG A Framework for Fine-Tuning Retrieval-Augmented Generation Systems.pdf:application/pdf;Snapshot:C\:\\Users\\guiba\\Zotero\\storage\\PJSLD6Q8\\2506.html:text/html},
}

@incollection{sofia_federated_2023,
	address = {New York},
	edition = {1},
	title = {Federated {Learning} {Models} in {Decentralized} {Critical} {Infrastructure}},
	copyright = {http://creativecommons.org/licenses/by-nc/4.0},
	isbn = {978-1-032-63240-7},
	url = {https://www.taylorfrancis.com/books/9781032632407/chapters/10.1201/9781032632407-7},
	language = {en},
	urldate = {2025-10-23},
	booktitle = {Shaping the {Future} of {IoT} with {Edge} {Intelligence}},
	publisher = {River Publishers},
	author = {Siniosoglou, Ilias and Bibi, Stamatia and Kollias, Konstantinos-Filippos and Fragulis, George and Radoglou-Grammatikis, Panagiotis and Lagkas, Thomas and Argyriou, Vasileios and Vitsas, Vasileios and Sarigiannidis, Panagiotis},
	collaborator = {Sofia, Rute C. and Soldatos, John},
	month = nov,
	year = {2023},
	doi = {10.1201/9781032632407-7},
	keywords = {Lido},
	pages = {95--115},
	annote = {Overview mais detalhado.
},
	file = {PDF:C\:\\Users\\guiba\\Zotero\\storage\\A8LKVAI7\\Siniosoglou et al. - 2023 - Federated Learning Models in Decentralized Critical Infrastructure.pdf:application/pdf},
}

@misc{guo_can_2025,
	title = {Can {Federated} {Learning} {Safeguard} {Private} {Data} in {LLM} {Training}? {Vulnerabilities}, {Attacks}, and {Defense} {Evaluation}},
	shorttitle = {Can {Federated} {Learning} {Safeguard} {Private} {Data} in {LLM} {Training}?},
	url = {http://arxiv.org/abs/2509.20680},
	doi = {10.48550/arXiv.2509.20680},
	abstract = {Fine-tuning large language models (LLMs) with local data is a widely adopted approach for organizations seeking to adapt LLMs to their specific domains. Given the shared characteristics in data across different organizations, the idea of collaboratively fine-tuning an LLM using data from multiple sources presents an appealing opportunity. However, organizations are often reluctant to share local data, making centralized fine-tuning impractical. Federated learning (FL), a privacy-preserving framework, enables clients to retain local data while sharing only model parameters for collaborative training, offering a potential solution. While fine-tuning LLMs on centralized datasets risks data leakage through next-token prediction, the iterative aggregation process in FL results in a global model that encapsulates generalized knowledge, which some believe protects client privacy. In this paper, however, we present contradictory findings through extensive experiments. We show that attackers can still extract training data from the global model, even using straightforward generation methods, with leakage increasing as the model size grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which tracks global model updates during training to intensify privacy leakage. To mitigate these risks, we evaluate privacy-preserving techniques in FL, including differential privacy, regularization-constrained updates and adopting LLMs with safety alignment. Our results provide valuable insights and practical guidelines for reducing privacy risks when training LLMs with FL.},
	language = {en},
	urldate = {2025-10-23},
	publisher = {arXiv},
	author = {Guo, Wenkai and Liu, Xuefeng and Wang, Haolin and Niu, Jianwei and Tang, Shaojie and Yuan, Jing},
	month = sep,
	year = {2025},
	note = {arXiv:2509.20680 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	annote = {Comment: 28 pages, 32 figures, accepted to the Findings of EMNLP 2025},
	file = {PDF:C\:\\Users\\guiba\\Zotero\\storage\\CVQ6SVQY\\Guo et al. - 2025 - Can Federated Learning Safeguard Private Data in LLM Training Vulnerabilities, Attacks, and Defense.pdf:application/pdf},
}

@misc{reddi_adaptive_2021,
	title = {Adaptive {Federated} {Optimization}},
	url = {http://arxiv.org/abs/2003.00295},
	doi = {10.48550/arXiv.2003.00295},
	abstract = {Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general non-convex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.},
	urldate = {2025-10-24},
	publisher = {arXiv},
	author = {Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Konečný, Jakub and Kumar, Sanjiv and McMahan, H. Brendan},
	month = sep,
	year = {2021},
	note = {arXiv:2003.00295 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Mathematics - Optimization and Control},
	annote = {Comment: Published as a conference paper at ICLR 2021},
	file = {Preprint PDF:C\:\\Users\\guiba\\Zotero\\storage\\SI5USVVF\\Reddi et al. - 2021 - Adaptive Federated Optimization.pdf:application/pdf;Snapshot:C\:\\Users\\guiba\\Zotero\\storage\\LYFM7RGF\\2003.html:text/html},
}

@misc{brasil_lei_2008,
	title = {{LEI} {Nº} 13.709, de 14 de {Agosto} de 2018},
	url = {https://www.planalto.gov.br/ccivil_03/_ato2015-2018/2018/lei/l13709.htm},
	abstract = {Lei Geral de Proteção de Dados Pessoais (LGPD).},
	urldate = {2025-11-11},
	author = {{Brasil}},
	month = aug,
	year = {2008},
}

@article{mcmahan_communication-efcient_2017,
	title = {Communication-{Efﬁcient} {Learning} of {Deep} {Networks} from {Decentralized} {Data}},
	abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering ﬁve different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a deﬁning characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10–100× as compared to synchronized stochastic gradient descent.},
	language = {en},
	author = {McMahan, H Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth},
	year = {2017},
	file = {PDF:C\:\\Users\\guiba\\Zotero\\storage\\ELAQABW5\\McMahan et al. - Communication-Efﬁcient Learning of Deep Networks from Decentralized Data.pdf:application/pdf},
}

@article{patil_review_2024,
	title = {A {Review} of {Current} {Trends}, {Techniques}, and {Challenges} in {Large} {Language} {Models} ({LLMs})},
	volume = {14},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/5/2074},
	doi = {10.3390/app14052074},
	abstract = {Natural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM’s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model’s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.},
	language = {en},
	number = {5},
	urldate = {2025-11-11},
	journal = {Applied Sciences},
	author = {Patil, Rajvardhan and Gudivada, Venkat},
	month = mar,
	year = {2024},
	pages = {2074},
	file = {PDF:C\:\\Users\\guiba\\Zotero\\storage\\7IJN99KK\\Patil e Gudivada - 2024 - A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs).pdf:application/pdf},
}

@article{anisuzzaman_fine-tuning_2025,
	title = {Fine-{Tuning} {Large} {Language} {Models} for {Specialized} {Use} {Cases}},
	volume = {3},
	issn = {29497612},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2949761224001147},
	doi = {10.1016/j.mcpdig.2024.11.005},
	language = {en},
	number = {1},
	urldate = {2025-11-13},
	journal = {Mayo Clinic Proceedings: Digital Health},
	author = {Anisuzzaman, D.M. and Malins, Jeffrey G. and Friedman, Paul A. and Attia, Zachi I.},
	month = mar,
	year = {2025},
	pages = {100184},
	file = {PDF:C\:\\Users\\guiba\\Zotero\\storage\\P7JIFXJK\\Anisuzzaman et al. - 2025 - Fine-Tuning Large Language Models for Specialized Use Cases.pdf:application/pdf},
}

@misc{zhang2024tinyllama,
      title={TinyLlama: An Open-Source Small Language Model}, 
      author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
      year={2024},
      eprint={2401.02385},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{peng2023instructiontuninggpt4,
      title={Instruction Tuning with GPT-4}, 
      author={Baolin Peng and Chunyuan Li and Pengcheng He and Michel Galley and Jianfeng Gao},
      year={2023},
      eprint={2304.03277},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.03277}, 
}

@inproceedings{carlini2021,
  title={Extracting Training Data from Large Language Models},
  author={Carlini, N. and others},
  booktitle={30th USENIX Security Symposium},
  pages={2633--2650},
  year={2021}
}

@article{levenshtein1966,
  title={Binary codes capable of correcting deletions, insertions, and reversals},
  author={Levenshtein, V. I.},
  journal={Soviet physics doklady},
  volume={10},
  pages={707--710},
  year={1966}
}

@inproceedings{reimers2019,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, N. and Gurevych, I.},
  booktitle={EMNLP-IJCNLP},
  pages={3982--3992},
  year={2019}
}